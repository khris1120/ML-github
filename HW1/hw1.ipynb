{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **HW1: Regression**\n",
        "In *assignment 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1873,
      "metadata": {
        "id": "HL5XjqFf4wSj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1874,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1875,
      "metadata": {
        "id": "OUbS2BEgcut6"
      },
      "outputs": [],
      "source": [
        "training_dataset = []\n",
        "validation_dataset = []\n",
        "w = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1876,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1877,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "    global training_dataset\n",
        "    global validation_dataset\n",
        "    global training_datalist\n",
        "    training_dataset = training_datalist[1:301]\n",
        "    validation_dataset = training_datalist[301:]\n",
        "\n",
        "SplitData()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1878,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def PreprocessData():\n",
        "    global training_dataset\n",
        "    global validation_dataset\n",
        "    training_dbp = training_dataset[:, 0].astype(float)\n",
        "    training_sbp = training_dataset[:, 1].astype(float)\n",
        "    validation_dbp = validation_dataset[:, 0].astype(float)\n",
        "    validation_sbp = validation_dataset[:, 1].astype(float)\n",
        "    training_dbp_mean = np.mean(training_dbp)\n",
        "    training_dbp_std = np.std(training_dbp)\n",
        "    training_sdp_mean = np.mean(training_sbp)\n",
        "    training_sdp_std = np.std(training_sbp)\n",
        "    validation_dbp_mean = np.mean(validation_dbp)\n",
        "    validation_dbp_std = np.std(validation_dbp)\n",
        "    validation_sbp_mean = np.mean(validation_sbp)\n",
        "    validation_sbp_std = np.std(validation_sbp)\n",
        "    training_dataset = training_dataset[(training_sbp >= training_sdp_mean - 1 * training_sdp_std) & \n",
        "                                         (training_sbp <= training_sdp_mean + 1 * training_sdp_std) & \n",
        "                                         (training_dbp >= training_dbp_mean - 1 * training_dbp_std) & \n",
        "                                         (training_dbp <= training_dbp_mean + 1 * training_dbp_std)]\n",
        "    validation_dataset = validation_dataset[(validation_sbp >= validation_sbp_mean - 1 * validation_sbp_std) & \n",
        "                                             (validation_sbp <= validation_sbp_mean + 1 * validation_sbp_std) & \n",
        "                                             (validation_dbp >= validation_dbp_mean - 1 * validation_dbp_std) & \n",
        "                                             (validation_dbp <= validation_dbp_mean + 1 * validation_dbp_std)]\n",
        "    \n",
        "PreprocessData()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1879,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.298458938732301\n"
          ]
        }
      ],
      "source": [
        "def MatrixInversion():\n",
        "    global training_dataset\n",
        "    global validation_dataset\n",
        "    global w\n",
        "\n",
        "    training_dpt = []\n",
        "    training_spt = []\n",
        "\n",
        "    for i, j in training_dataset:\n",
        "        training_dpt.append([float(i)])\n",
        "        training_spt.append([float(j)])\n",
        "\n",
        "    training_dpt = np.array(training_dpt)\n",
        "    training_spt = np.array(training_spt)\n",
        "\n",
        "    valid_dpt = []\n",
        "    valid_spt = []\n",
        "\n",
        "    for i, j in validation_dataset:\n",
        "        valid_dpt.append([float(i)])\n",
        "        valid_spt.append([float(j)])\n",
        "\n",
        "    valid_dpt = np.array(valid_dpt)\n",
        "    valid_spt = np.array(valid_spt)\n",
        "\n",
        "    training_dpt_transpose = np.transpose(training_dpt)\n",
        "    tmp = np.dot(training_dpt_transpose, training_dpt)\n",
        "    tmp_inv = np.linalg.inv(tmp)\n",
        "    w = np.dot(np.dot(tmp_inv,training_dpt_transpose),training_spt)\n",
        "\n",
        "    w = np.array(w)\n",
        "    prediction = np.dot(valid_dpt, w)\n",
        "    mape = np.mean(np.abs((valid_spt - prediction) / valid_spt)) * 100\n",
        "    print(mape)\n",
        "\n",
        "MatrixInversion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1880,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[146.25555726049816], [130.69645542427494], [129.1405452406526], [157.1469285458544], [96.46643138458388], [154.03510817860976], [141.5878267096312], [141.5878267096312], [132.25236560789727], [122.91690450616335], [121.36099432254102], [126.02872487340798], [110.46962303718477], [121.36099432254102], [144.69964707687583], [136.92009615876424], [164.92647946396602], [146.25555726049816], [149.3673776277428], [160.25874891309905]]\n"
          ]
        }
      ],
      "source": [
        "def MakePrediction():\n",
        "    global testing_datalist\n",
        "    global w\n",
        "    global output_datalist\n",
        "\n",
        "    testing_dpt = []\n",
        "    for i in testing_datalist[1:]:\n",
        "        testing_dpt.append([float(i[0])])\n",
        "    \n",
        "    testing_dpt = np.array(testing_dpt)\n",
        "    testing_spt = np.dot(testing_dpt, w)\n",
        "    output_datalist = testing_spt.tolist()\n",
        "    print(output_datalist)\n",
        "\n",
        "MakePrediction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1881,
      "metadata": {
        "id": "iCL92EPKOFIn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.5559101836223208\n"
          ]
        }
      ],
      "source": [
        "print(float(w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1882,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1883,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']\n",
        "\n",
        "coefficient_output = [] # Your coefficient update during gradient descent\n",
        "                   # Should be a (number of iterations * number_of coefficient) matrix\n",
        "                   # The format of each row should be ['w0', 'w1', ...., 'wn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1884,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": [
        "training_dataset = []\n",
        "validation_dataset = []\n",
        "w = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1885,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "    global training_dataset\n",
        "    global validation_dataset\n",
        "    global training_datalist\n",
        "    training_dataset = training_datalist[1:301]\n",
        "    validation_dataset = training_datalist[301:]\n",
        "\n",
        "SplitData()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1886,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "def PreprocessData():\n",
        "    global training_dataset\n",
        "    global validation_dataset\n",
        "    training_dbp = training_dataset[:, 0].astype(float)\n",
        "    training_sbp = training_dataset[:, 1].astype(float)\n",
        "    validation_dbp = validation_dataset[:, 0].astype(float)\n",
        "    validation_sbp = validation_dataset[:, 1].astype(float)\n",
        "    training_dbp_mean = np.mean(training_dbp)\n",
        "    training_dbp_std = np.std(training_dbp)\n",
        "    training_sdp_mean = np.mean(training_sbp)\n",
        "    training_sdp_std = np.std(training_sbp)\n",
        "    validation_dbp_mean = np.mean(validation_dbp)\n",
        "    validation_dbp_std = np.std(validation_dbp)\n",
        "    validation_sbp_mean = np.mean(validation_sbp)\n",
        "    validation_sbp_std = np.std(validation_sbp)\n",
        "    training_dataset = training_dataset[(training_sbp >= training_sdp_mean - 1 * training_sdp_std) & \n",
        "                                         (training_sbp <= training_sdp_mean + 1 * training_sdp_std) & \n",
        "                                         (training_dbp >= training_dbp_mean - 1 * training_dbp_std) & \n",
        "                                         (training_dbp <= training_dbp_mean + 1 * training_dbp_std)]\n",
        "    validation_dataset = validation_dataset[(validation_sbp >= validation_sbp_mean - 1 * validation_sbp_std) & \n",
        "                                             (validation_sbp <= validation_sbp_mean + 1 * validation_sbp_std) & \n",
        "                                             (validation_dbp >= validation_dbp_mean - 1 * validation_dbp_std) & \n",
        "                                             (validation_dbp <= validation_dbp_mean + 1 * validation_dbp_std)]\n",
        "    \n",
        "PreprocessData()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1887,
      "metadata": {
        "id": "-635Ee00YHTE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.555910183622321\n",
            "mape: 5.298458938732301\n",
            "coefficient_output: [[0], array([2.1581473]), array([1.32280576]), array([1.64613656]), array([1.52098679]), array([1.56942778]), array([1.55067801]), array([1.55793537]), array([1.55512631]), array([1.55621359]), array([1.55579274]), array([1.55595564]), array([1.55589259]), array([1.55591699]), array([1.55590755]), array([1.5559112]), array([1.55590979]), array([1.55591034]), array([1.55591012]), array([1.55591021]), array([1.55591017]), array([1.55591019]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018]), array([1.55591018])]\n"
          ]
        }
      ],
      "source": [
        "def GradientDescent():\n",
        "    global training_dataset\n",
        "    global validation_dataset\n",
        "    global w\n",
        "    global coefficient_output\n",
        "\n",
        "    training_dpt = []\n",
        "    training_spt = []\n",
        "\n",
        "    for i, j in training_dataset:\n",
        "        training_dpt.append([float(i)])\n",
        "        training_spt.append([float(j)])\n",
        "\n",
        "    training_dpt = np.array(training_dpt)\n",
        "    training_spt = np.array(training_spt)\n",
        "\n",
        "    valid_dpt = []\n",
        "    valid_spt = []\n",
        "\n",
        "    for i, j in validation_dataset:\n",
        "        valid_dpt.append([float(i)])\n",
        "        valid_spt.append([float(j)])\n",
        "\n",
        "    learning_rate = 0.0001\n",
        "    n_iterations = 1000\n",
        "\n",
        "    w = 0\n",
        "    coefficient_output.append([w])\n",
        "\n",
        "    for iteration in range(n_iterations):\n",
        "        # Calculate predictions\n",
        "        spt_pred = training_dpt.dot(w)\n",
        "        \n",
        "        # Calculate the gradient of the Mean Squared Error\n",
        "        gradient = -2 * training_dpt.T.dot(training_spt - spt_pred) / training_dpt.shape[0]\n",
        "        \n",
        "        # Update the coefficients\n",
        "        w -= learning_rate * gradient\n",
        "        coefficient_output.append(w.flatten())\n",
        "\n",
        "    print(float(w))\n",
        "\n",
        "    w = np.array(w)\n",
        "    prediction = np.dot(valid_dpt, w)\n",
        "    mape = np.mean(np.abs((valid_spt - prediction) / valid_spt)) * 100\n",
        "    print(\"mape:\", mape)\n",
        "\n",
        "    print(\"coefficient_output:\", coefficient_output)\n",
        "\n",
        "    \n",
        "\n",
        "GradientDescent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1888,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[146.25555726049816], [130.69645542427497], [129.14054524065264], [157.14692854585442], [96.4664313845839], [154.0351081786098], [141.5878267096312], [141.5878267096312], [132.25236560789727], [122.91690450616336], [121.36099432254105], [126.02872487340801], [110.46962303718479], [121.36099432254105], [144.69964707687586], [136.92009615876424], [164.92647946396602], [146.25555726049816], [149.36737762774283], [160.25874891309905]]\n"
          ]
        }
      ],
      "source": [
        "def MakePrediction():\n",
        "    global testing_datalist\n",
        "    global w\n",
        "    global output_datalist\n",
        "\n",
        "    testing_dpt = []\n",
        "    for i in testing_datalist[1:]:\n",
        "        testing_dpt.append([float(i[0])])\n",
        "    \n",
        "    testing_dpt = np.array(testing_dpt)\n",
        "    testing_spt = np.dot(testing_dpt, w)\n",
        "    output_datalist = testing_spt.tolist()\n",
        "    print(output_datalist)\n",
        "\n",
        "MakePrediction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1889,
      "metadata": {
        "id": "90EisOc7YG-N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.555910183622321\n"
          ]
        }
      ],
      "source": [
        "print(float(w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1890,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)\n",
        "\n",
        "with open(coefficient_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in coefficient_output:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1891,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1892,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mape: [11.83468206]\n"
          ]
        }
      ],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "def SplitData():\n",
        "    global training_dataset\n",
        "    global validation_dataset\n",
        "    global training_datalist\n",
        "    \n",
        "    training_datalist = training_datalist[1:]\n",
        "    np.random.shuffle(training_datalist)\n",
        "    training_dataset = training_datalist[1:]\n",
        "    validation_dataset = training_datalist[200:500]\n",
        "\n",
        "def PreprocessData():\n",
        "   global training_dataset\n",
        "   global validation_dataset\n",
        "   global testing_datalist\n",
        "   testing_datalist = testing_datalist[1:]\n",
        "   #avg_spt = np.mean(training_dataset[:, 6].astype(float))\n",
        "   for i, data in enumerate(training_dataset):\n",
        "      if (data[2] == '' or (float(data[2]) >= 98 or float(data[2]) <= 96)):\n",
        "         data[2] = 97\n",
        "      if (data[3] == '' or (float(data[3]) >= 98 or float(data[3]) <= 72)):\n",
        "         data[3] = 85\n",
        "      if (data[4] == '' or (float(data[4]) >= 19.5 or float(data[4]) <= 14.5)):\n",
        "         data[4] = 17\n",
        "      if (data[5] == '' or (float(data[5]) >= 101 or float(data[5]) <= 93)):\n",
        "         data[5] = 97\n",
        "      training_dataset[i] = data\n",
        "\n",
        "   for i, data in enumerate(validation_dataset):\n",
        "      if (data[2] == '' or (float(data[2]) >= 106 or float(data[2]) <= 90)):\n",
        "         data[2] = 97\n",
        "      if (data[3] == '' or (float(data[3]) >= 145 or float(data[3]) <= 40)):\n",
        "         data[3] = 85\n",
        "      if (data[4] == '' or (float(data[4]) >= 24 or float(data[4]) <= 10)):\n",
        "         data[4] = 17\n",
        "      if (data[5] == '' or (float(data[5]) >= 110 or float(data[5]) <= 85)):\n",
        "         data[5] = 97\n",
        "      validation_dataset[i] = data\n",
        "\n",
        "SplitData()\n",
        "PreprocessData()\n",
        "\n",
        "def sigmoid(x):\n",
        "   return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def GradientDescent():\n",
        "   global training_dataset\n",
        "\n",
        "   training_phi1 = []\n",
        "   training_phi2 = []\n",
        "   training_phi3 = []\n",
        "   training_phi4 = []\n",
        "   training_phi5 = []\n",
        "   training_phi6 = []\n",
        "   training_phi7 = []\n",
        "   training_phi8 = []\n",
        "   training_phi9 = []\n",
        "   training_phi10 = []\n",
        "   training_phi11 = []\n",
        "   valid_phi = []\n",
        "   # valid_phi2 = []\n",
        "   # valid_phi3 = []\n",
        "   # valid_phi4 = []\n",
        "   # valid_phi5 = []\n",
        "   # valid_phi6 = []\n",
        "   # valid_phi7 = []\n",
        "   # valid_phi8 = []\n",
        "   # valid_phi9 = []\n",
        "   # valid_phi10 = []\n",
        "   # valid_phi11 = []\n",
        "\n",
        "   training_spt1 = []\n",
        "   training_spt2 = []\n",
        "   training_spt3 = []\n",
        "   training_spt4 = []\n",
        "   training_spt5 = []\n",
        "   training_spt6 = []\n",
        "   training_spt7 = []\n",
        "   training_spt8 = []\n",
        "   training_spt9 = []\n",
        "   training_spt10 = []\n",
        "   training_spt11 = []\n",
        "   valid_spt = []\n",
        "\n",
        "\n",
        "   temp_avg = np.mean(training_dataset[:, 2].astype(float))\n",
        "   temp_std = np.std(training_dataset[:, 2].astype(float))\n",
        "   heart_avg = np.mean(training_dataset[:, 3].astype(float))\n",
        "   heart_std = np.std(training_dataset[:, 3].astype(float))\n",
        "   resp_avg = np.mean(training_dataset[:, 4].astype(float))\n",
        "   resp_std = np.std(training_dataset[:, 4].astype(float))\n",
        "   o2_avg = np.mean(training_dataset[:, 5].astype(float))\n",
        "   o2_std = np.std(training_dataset[:, 5].astype(float))\n",
        "   training_dataset_num = len(training_dataset)\n",
        "   validation_dataset_num = len(validation_dataset)\n",
        "\n",
        "   for i in range(training_dataset_num):\n",
        "      if (training_dataset[i][0] == '11526383'):\n",
        "         training_phi1.append([1, sigmoid((float(training_dataset[i][2]) - temp_avg) / temp_std),\n",
        "                                 sigmoid((float(training_dataset[i][3]) - heart_avg) / heart_std), \n",
        "                                 sigmoid((float(training_dataset[i][4]) - resp_avg) / resp_std),\n",
        "                                 sigmoid((float(training_dataset[i][5]) - o2_avg) / o2_std)])\n",
        "         training_spt1.append([float(training_dataset[i][6])])\n",
        "      elif (training_dataset[i][0] == '12923910'):\n",
        "         training_phi2.append([1, sigmoid((float(training_dataset[i][2]) - temp_avg) / temp_std),\n",
        "                                 sigmoid((float(training_dataset[i][3]) - heart_avg) / heart_std), \n",
        "                                 sigmoid((float(training_dataset[i][4]) - resp_avg) / resp_std),\n",
        "                                 sigmoid((float(training_dataset[i][5]) - o2_avg) / o2_std)])\n",
        "         training_spt2.append([float(training_dataset[i][6])])\n",
        "      elif (training_dataset[i][0] == '14699420'):\n",
        "         training_phi3.append([1, sigmoid((float(training_dataset[i][2]) - temp_avg) / temp_std),\n",
        "                                 sigmoid((float(training_dataset[i][3]) - heart_avg) / heart_std), \n",
        "                                 sigmoid((float(training_dataset[i][4]) - resp_avg) / resp_std),\n",
        "                                 sigmoid((float(training_dataset[i][5]) - o2_avg) / o2_std)])\n",
        "         training_spt3.append([float(training_dataset[i][6])])\n",
        "      elif (training_dataset[i][0] == '15437705'):\n",
        "         training_phi4.append([1, sigmoid((float(training_dataset[i][2]) - temp_avg) / temp_std),\n",
        "                                 sigmoid((float(training_dataset[i][3]) - heart_avg) / heart_std), \n",
        "                                 sigmoid((float(training_dataset[i][4]) - resp_avg) / resp_std),\n",
        "                                 sigmoid((float(training_dataset[i][5]) - o2_avg) / o2_std)])\n",
        "         training_spt4.append([float(training_dataset[i][6])])\n",
        "      elif (training_dataset[i][0] == '15642911'):\n",
        "         training_phi5.append([1, sigmoid((float(training_dataset[i][2]) - temp_avg) / temp_std),\n",
        "                                 sigmoid((float(training_dataset[i][3]) - heart_avg) / heart_std), \n",
        "                                 sigmoid((float(training_dataset[i][4]) - resp_avg) / resp_std),\n",
        "                                 sigmoid((float(training_dataset[i][5]) - o2_avg) / o2_std)])\n",
        "         training_spt5.append([float(training_dataset[i][6])])\n",
        "      elif (training_dataset[i][0] == '16298357'):\n",
        "         training_phi6.append([1, sigmoid((float(training_dataset[i][2]) - temp_avg) / temp_std),\n",
        "                                 sigmoid((float(training_dataset[i][3]) - heart_avg) / heart_std), \n",
        "                                 sigmoid((float(training_dataset[i][4]) - resp_avg) / resp_std),\n",
        "                                 sigmoid((float(training_dataset[i][5]) - o2_avg) / o2_std)])\n",
        "         training_spt6.append([float(training_dataset[i][6])])\n",
        "      elif (training_dataset[i][0] == '17331999'):\n",
        "         training_phi7.append([1, sigmoid((float(training_dataset[i][2]) - temp_avg) / temp_std),\n",
        "                                 sigmoid((float(training_dataset[i][3]) - heart_avg) / heart_std), \n",
        "                                 sigmoid((float(training_dataset[i][4]) - resp_avg) / resp_std),\n",
        "                                 sigmoid((float(training_dataset[i][5]) - o2_avg) / o2_std)])\n",
        "         training_spt7.append([float(training_dataset[i][6])])\n",
        "      elif (training_dataset[i][0] == '17593883'):\n",
        "         training_phi8.append([1, sigmoid((float(training_dataset[i][2]) - temp_avg) / temp_std),\n",
        "                                 sigmoid((float(training_dataset[i][3]) - heart_avg) / heart_std), \n",
        "                                 sigmoid((float(training_dataset[i][4]) - resp_avg) / resp_std),\n",
        "                                 sigmoid((float(training_dataset[i][5]) - o2_avg) / o2_std)])\n",
        "         training_spt8.append([float(training_dataset[i][6])])\n",
        "      elif (training_dataset[i][0] == '18733920'):\n",
        "         training_phi9.append([1, sigmoid((float(training_dataset[i][2]) - temp_avg) / temp_std),\n",
        "                                 sigmoid((float(training_dataset[i][3]) - heart_avg) / heart_std), \n",
        "                                 sigmoid((float(training_dataset[i][4]) - resp_avg) / resp_std),\n",
        "                                 sigmoid((float(training_dataset[i][5]) - o2_avg) / o2_std)])\n",
        "         training_spt9.append([float(training_dataset[i][6])])\n",
        "      elif (training_dataset[i][0] == '18791093'):\n",
        "         training_phi10.append([1, sigmoid((float(training_dataset[i][2]) - temp_avg) / temp_std),\n",
        "                                 sigmoid((float(training_dataset[i][3]) - heart_avg) / heart_std), \n",
        "                                 sigmoid((float(training_dataset[i][4]) - resp_avg) / resp_std),\n",
        "                                 sigmoid((float(training_dataset[i][5]) - o2_avg) / o2_std)])\n",
        "         training_spt10.append([float(training_dataset[i][6])])\n",
        "      elif (training_dataset[i][0] == '19473413'):\n",
        "         training_phi11.append([1, sigmoid((float(training_dataset[i][2]) - temp_avg) / temp_std),\n",
        "                                 sigmoid((float(training_dataset[i][3]) - heart_avg) / heart_std), \n",
        "                                 sigmoid((float(training_dataset[i][4]) - resp_avg) / resp_std),\n",
        "                                 sigmoid((float(training_dataset[i][5]) - o2_avg) / o2_std)])\n",
        "         training_spt11.append([float(training_dataset[i][6])])\n",
        "\n",
        "   training_spt1, training_spt2, training_spt3, training_spt4, training_spt5, training_spt6, training_spt7, training_spt8, training_spt9, training_spt10, training_spt11 = np.array(training_spt1), np.array(training_spt2), np.array(training_spt3), np.array(training_spt4), np.array(training_spt5), np.array(training_spt6), np.array(training_spt7), np.array(training_spt8), np.array(training_spt9), np.array(training_spt10), np.array(training_spt11)\n",
        "\n",
        "   for i in range (validation_dataset_num):\n",
        "      valid_phi.append([validation_dataset[i][0], 1, \n",
        "                        sigmoid((float(validation_dataset[i][2]) - temp_avg) / temp_std),\n",
        "                        sigmoid((float(validation_dataset[i][3]) - heart_avg) / heart_std), \n",
        "                        sigmoid((float(validation_dataset[i][4]) - resp_avg) / resp_std),\n",
        "                        sigmoid((float(validation_dataset[i][5]) - o2_avg) / o2_std)])\n",
        "      valid_spt.append([float(validation_dataset[i][6])])\n",
        "   \n",
        "   training_phi1, training_phi2, training_phi3, training_phi4, training_phi5, training_phi6, training_phi7, training_phi8, training_phi9, training_phi10, training_phi11 = np.array(training_phi1), np.array(training_phi2), np.array(training_phi3), np.array(training_phi4), np.array(training_phi5), np.array(training_phi6), np.array(training_phi7), np.array(training_phi8), np.array(training_phi9), np.array(training_phi10), np.array(training_phi11)\n",
        "   valid_phi = np.array(valid_phi).astype(float)\n",
        "   valid_spt = np.array(valid_spt).astype(float)\n",
        "   learning_rate = 0.1\n",
        "   n_iterations = 100000\n",
        "\n",
        "   w1 = [[0], [0], [0], [0], [0]]\n",
        "   w2 = [[0], [0], [0], [0], [0]]\n",
        "   w3 = [[0], [0], [0], [0], [0]]\n",
        "   w4 = [[0], [0], [0], [0], [0]]\n",
        "   w5 = [[0], [0], [0], [0], [0]]\n",
        "   w6 = [[0], [0], [0], [0], [0]]\n",
        "   w7 = [[0], [0], [0], [0], [0]]\n",
        "   w8 = [[0], [0], [0], [0], [0]]\n",
        "   w9 = [[0], [0], [0], [0], [0]]\n",
        "   w10 = [[0], [0], [0], [0], [0]]\n",
        "   w11 = [[0], [0], [0], [0], [0]]\n",
        "\n",
        "   for iteration in range(n_iterations):\n",
        "      \n",
        "      spt_pred1 = training_phi1.dot(w1)\n",
        "      gradient = -2 * training_phi1.T.dot(training_spt1 - spt_pred1) / training_phi1.shape[0]\n",
        "      w1 -= learning_rate * gradient\n",
        "\n",
        "      spt_pred2 = training_phi2.dot(w2)\n",
        "      gradient = -2 * training_phi2.T.dot(training_spt2 - spt_pred2) / training_phi2.shape[0]\n",
        "      w2 -= learning_rate * gradient\n",
        "\n",
        "      spt_pred3 = training_phi3.dot(w3)\n",
        "      gradient = -2 * training_phi3.T.dot(training_spt3 - spt_pred3) / training_phi3.shape[0]\n",
        "      w3 -= learning_rate * gradient\n",
        "\n",
        "      spt_pred4 = training_phi4.dot(w4)\n",
        "      gradient = -2 * training_phi4.T.dot(training_spt4 - spt_pred4) / training_phi4.shape[0]\n",
        "      w4 -= learning_rate * gradient\n",
        "\n",
        "      spt_pred5 = training_phi5.dot(w5)\n",
        "      gradient = -2 * training_phi5.T.dot(training_spt5 - spt_pred5) / training_phi5.shape[0]\n",
        "      w5 -= learning_rate * gradient\n",
        "\n",
        "      spt_pred6 = training_phi6.dot(w6)\n",
        "      gradient = -2 * training_phi6.T.dot(training_spt6 - spt_pred6) / training_phi6.shape[0]\n",
        "      w6 -= learning_rate * gradient\n",
        "\n",
        "      spt_pred7 = training_phi7.dot(w7)\n",
        "      gradient = -2 * training_phi7.T.dot(training_spt7 - spt_pred7) / training_phi7.shape[0]\n",
        "      w7 -= learning_rate * gradient\n",
        "\n",
        "      spt_pred8 = training_phi8.dot(w8)\n",
        "      gradient = -2 * training_phi8.T.dot(training_spt8 - spt_pred8) / training_phi8.shape[0]\n",
        "      w8 -= learning_rate * gradient\n",
        "\n",
        "      spt_pred9 = training_phi9.dot(w9)\n",
        "      gradient = -2 * training_phi9.T.dot(training_spt9 - spt_pred9) / training_phi9.shape[0]\n",
        "      w9 -= learning_rate * gradient\n",
        "\n",
        "      spt_pred10 = training_phi10.dot(w10)\n",
        "      gradient = -2 * training_phi10.T.dot(training_spt10 - spt_pred10) / training_phi10.shape[0]\n",
        "      w10 -= learning_rate * gradient\n",
        "\n",
        "      spt_pred11 = training_phi11.dot(w11)\n",
        "      gradient = -2 * training_phi11.T.dot(training_spt11 - spt_pred11) / training_phi11.shape[0]\n",
        "      w11 -= learning_rate * gradient\n",
        "\n",
        "   w1, w2, w3, w4, w5, w6, w7, w8, w9, w10, w11 = np.array(w1), np.array(w2), np.array(w3), np.array(w4), np.array(w5), np.array(w6), np.array(w7), np.array(w8), np.array(w9), np.array(w10), np.array(w11)\n",
        "   \n",
        "   mape = 0\n",
        "   for i, data in enumerate(valid_phi):\n",
        "      if (data[0] == 11526383):\n",
        "         prediction = np.dot(data[1:], w1)\n",
        "         mape += np.abs((valid_spt[i] - prediction) / valid_spt[i])\n",
        "      elif (data[0] == 12923910):\n",
        "         prediction = np.dot(data[1:], w2)\n",
        "         mape += np.abs((valid_spt[i] - prediction) / valid_spt[i])\n",
        "      elif (data[0] == 14699420):\n",
        "         prediction = np.dot(data[1:], w3)\n",
        "         mape += np.abs((valid_spt[i] - prediction) / valid_spt[i])\n",
        "      elif (data[0] == 15437705):\n",
        "         prediction = np.dot(data[1:], w4)\n",
        "         mape += np.abs((valid_spt[i] - prediction) / valid_spt[i])\n",
        "      elif (data[0] == 15642911):\n",
        "         prediction = np.dot(data[1:], w5)\n",
        "         mape += np.abs((valid_spt[i] - prediction) / valid_spt[i])\n",
        "      elif (data[0] == 16298357):\n",
        "         prediction = np.dot(data[1:], w6)\n",
        "         mape += np.abs((valid_spt[i] - prediction) / valid_spt[i])\n",
        "      elif (data[0] == 17331999):\n",
        "         prediction = np.dot(data[1:], w7)\n",
        "         mape += np.abs((valid_spt[i] - prediction) / valid_spt[i])\n",
        "      elif (data[0] == 17593883):\n",
        "         prediction = np.dot(data[1:], w8)\n",
        "         mape += np.abs((valid_spt[i] - prediction) / valid_spt[i])\n",
        "      elif (data[0] == 18733920):\n",
        "         prediction = np.dot(data[1:], w9)\n",
        "         mape += np.abs((valid_spt[i] - prediction) / valid_spt[i])\n",
        "      elif (data[0] == 18791093):\n",
        "         prediction = np.dot(data[1:], w10)\n",
        "         mape += np.abs((valid_spt[i] - prediction) / valid_spt[i])\n",
        "      elif (data[0] == 19473413):\n",
        "         prediction = np.dot(data[1:], w11)\n",
        "         mape += np.abs((valid_spt[i] - prediction) / valid_spt[i])\n",
        "\n",
        "   mape /= validation_dataset_num\n",
        "   mape *= 100\n",
        "   print(\"mape:\", mape)\n",
        "\n",
        "   for i, data in enumerate(testing_datalist):\n",
        "      d = [1, sigmoid((float(testing_datalist[i][2]) - temp_avg) / temp_std),\n",
        "               sigmoid((float(testing_datalist[i][3]) - heart_avg) / heart_std), \n",
        "               sigmoid((float(testing_datalist[i][4]) - resp_avg) / resp_std),\n",
        "               sigmoid((float(testing_datalist[i][5]) - o2_avg) / o2_std)]\n",
        "      if (data[0] == '11526383'):\n",
        "         output_datalist.append(np.dot(d[:], w1)) \n",
        "      elif (data[0] == '12923910'):\n",
        "         output_datalist.append(np.dot(d[:], w2)) \n",
        "      elif (data[0] == '14699420'):\n",
        "         output_datalist.append(np.dot(d[:], w3)) \n",
        "      elif (data[0] == '15437705'):\n",
        "         output_datalist.append(np.dot(d[:], w4)) \n",
        "      elif (data[0] == '15642911'):\n",
        "         output_datalist.append(np.dot(d[:], w5)) \n",
        "      elif (data[0] == '16298357'):\n",
        "         output_datalist.append(np.dot(d[:], w6)) \n",
        "      elif (data[0] == '17331999'):\n",
        "         output_datalist.append(np.dot(d[:], w7)) \n",
        "      elif (data[0] == '17593883'):\n",
        "         output_datalist.append(np.dot(d[:], w8)) \n",
        "      elif (data[0] == '18733920'):\n",
        "         output_datalist.append(np.dot(d[:], w9)) \n",
        "      elif (data[0] == '18791093'):\n",
        "         output_datalist.append(np.dot(d[:], w10)) \n",
        "      elif (data[0] == '19473413'):\n",
        "         output_datalist.append(np.dot(d[:], w11)) \n",
        "      else:\n",
        "         print('hi')\n",
        "\n",
        "\n",
        "GradientDescent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1893,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
